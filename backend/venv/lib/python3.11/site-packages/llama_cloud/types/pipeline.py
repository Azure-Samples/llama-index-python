# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

from ..core.datetime_utils import serialize_datetime
from .configured_transformation_item import ConfiguredTransformationItem
from .data_sink import DataSink
from .eval_execution_params import EvalExecutionParams
from .llama_parse_parameters import LlamaParseParameters
from .pipeline_embedding_config import PipelineEmbeddingConfig
from .pipeline_transform_config import PipelineTransformConfig
from .pipeline_type import PipelineType
from .preset_retrieval_params import PresetRetrievalParams

try:
    import pydantic
    if pydantic.__version__.startswith("1."):
        raise ImportError
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore


class Pipeline(pydantic.BaseModel):
    """
    Schema for a pipeline.
    """

    id: str = pydantic.Field(description="Unique identifier")
    created_at: typing.Optional[dt.datetime] = pydantic.Field(description="Creation datetime")
    updated_at: typing.Optional[dt.datetime] = pydantic.Field(description="Update datetime")
    name: str
    project_id: str
    pipeline_type: typing.Optional[PipelineType] = pydantic.Field(
        description="Type of pipeline. Either PLAYGROUND or MANAGED."
    )
    managed_pipeline_id: typing.Optional[str] = pydantic.Field(
        description="The ID of the ManagedPipeline this playground pipeline is linked to."
    )
    embedding_config: typing.Optional[PipelineEmbeddingConfig] = pydantic.Field(
        description="Configuration for the embedding model."
    )
    configured_transformations: typing.Optional[typing.List[ConfiguredTransformationItem]] = pydantic.Field(
        description="Deprecated don't use it, List of configured transformations."
    )
    transform_config: typing.Optional[PipelineTransformConfig] = pydantic.Field(
        description="Configuration for the transformation."
    )
    preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = pydantic.Field(
        description="Preset retrieval parameters for the pipeline."
    )
    eval_parameters: typing.Optional[EvalExecutionParams] = pydantic.Field(
        description="Eval parameters for the pipeline."
    )
    llama_parse_parameters: typing.Optional[LlamaParseParameters] = pydantic.Field(
        description="Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline."
    )
    data_sink: typing.Optional[DataSink] = pydantic.Field(
        description="The data sink for the pipeline. If None, the pipeline will use the fully managed data sink."
    )

    def json(self, **kwargs: typing.Any) -> str:
        kwargs_with_defaults: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        return super().json(**kwargs_with_defaults)

    def dict(self, **kwargs: typing.Any) -> typing.Dict[str, typing.Any]:
        kwargs_with_defaults: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        return super().dict(**kwargs_with_defaults)

    class Config:
        frozen = True
        smart_union = True
        json_encoders = {dt.datetime: serialize_datetime}
