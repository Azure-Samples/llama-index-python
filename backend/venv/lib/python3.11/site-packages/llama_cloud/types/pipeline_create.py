# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

from ..core.datetime_utils import serialize_datetime
from .configured_transformation_item import ConfiguredTransformationItem
from .data_sink_create import DataSinkCreate
from .eval_execution_params import EvalExecutionParams
from .llama_parse_parameters import LlamaParseParameters
from .pipeline_create_embedding_config import PipelineCreateEmbeddingConfig
from .pipeline_create_transform_config import PipelineCreateTransformConfig
from .pipeline_type import PipelineType
from .preset_retrieval_params import PresetRetrievalParams

try:
    import pydantic
    if pydantic.__version__.startswith("1."):
        raise ImportError
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore


class PipelineCreate(pydantic.BaseModel):
    """
    Schema for creating a pipeline.
    """

    embedding_config: typing.Optional[PipelineCreateEmbeddingConfig] = pydantic.Field(
        description="Configuration for the embedding model."
    )
    transform_config: typing.Optional[PipelineCreateTransformConfig] = pydantic.Field(
        description="Configuration for the transformation."
    )
    configured_transformations: typing.Optional[typing.List[ConfiguredTransformationItem]] = pydantic.Field(
        description="Deprecated, use embedding_config or transform_config instead. configured transformations for the pipeline."
    )
    data_sink_id: typing.Optional[str] = pydantic.Field(
        description="Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID."
    )
    data_sink: typing.Optional[DataSinkCreate] = pydantic.Field(
        description="Data sink. When provided instead of data_sink_id, the data sink will be created."
    )
    preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = pydantic.Field(
        description="Preset retrieval parameters for the pipeline."
    )
    eval_parameters: typing.Optional[EvalExecutionParams] = pydantic.Field(
        description="Eval parameters for the pipeline."
    )
    llama_parse_parameters: typing.Optional[LlamaParseParameters] = pydantic.Field(
        description="Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline."
    )
    name: str
    pipeline_type: typing.Optional[PipelineType] = pydantic.Field(
        description="Type of pipeline. Either PLAYGROUND or MANAGED."
    )
    managed_pipeline_id: typing.Optional[str] = pydantic.Field(
        description="The ID of the ManagedPipeline this playground pipeline is linked to."
    )

    def json(self, **kwargs: typing.Any) -> str:
        kwargs_with_defaults: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        return super().json(**kwargs_with_defaults)

    def dict(self, **kwargs: typing.Any) -> typing.Dict[str, typing.Any]:
        kwargs_with_defaults: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        return super().dict(**kwargs_with_defaults)

    class Config:
        frozen = True
        smart_union = True
        json_encoders = {dt.datetime: serialize_datetime}
