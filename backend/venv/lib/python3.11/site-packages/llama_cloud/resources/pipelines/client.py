# This file was auto-generated by Fern from our API Definition.

import typing
import urllib.parse
from json.decoder import JSONDecodeError

from ...core.api_error import ApiError
from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.jsonable_encoder import jsonable_encoder
from ...core.remove_none_from_dict import remove_none_from_dict
from ...errors.unprocessable_entity_error import UnprocessableEntityError
from ...types.chat_data import ChatData
from ...types.cloud_document import CloudDocument
from ...types.cloud_document_create import CloudDocumentCreate
from ...types.configured_transformation_item import ConfiguredTransformationItem
from ...types.data_sink_create import DataSinkCreate
from ...types.eval_dataset_job_record import EvalDatasetJobRecord
from ...types.eval_execution_params import EvalExecutionParams
from ...types.eval_execution_params_override import EvalExecutionParamsOverride
from ...types.eval_question_result import EvalQuestionResult
from ...types.http_validation_error import HttpValidationError
from ...types.input_message import InputMessage
from ...types.llama_parse_parameters import LlamaParseParameters
from ...types.managed_ingestion_status_response import ManagedIngestionStatusResponse
from ...types.metadata_filters import MetadataFilters
from ...types.pipeline import Pipeline
from ...types.pipeline_create import PipelineCreate
from ...types.pipeline_data_source import PipelineDataSource
from ...types.pipeline_data_source_create import PipelineDataSourceCreate
from ...types.pipeline_deployment import PipelineDeployment
from ...types.pipeline_file import PipelineFile
from ...types.pipeline_file_create import PipelineFileCreate
from ...types.pipeline_type import PipelineType
from ...types.playground_session import PlaygroundSession
from ...types.preset_retrieval_params import PresetRetrievalParams
from ...types.retrieval_mode import RetrievalMode
from ...types.retrieve_results import RetrieveResults
from ...types.text_node import TextNode
from .types.pipeline_file_update_custom_metadata_value import PipelineFileUpdateCustomMetadataValue
from .types.pipeline_update_embedding_config import PipelineUpdateEmbeddingConfig
from .types.pipeline_update_transform_config import PipelineUpdateTransformConfig

try:
    import pydantic
    if pydantic.__version__.startswith("1."):
        raise ImportError
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class PipelinesClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def search_pipelines(
        self,
        *,
        project_name: typing.Optional[str] = None,
        pipeline_name: typing.Optional[str] = None,
        pipeline_type: typing.Optional[PipelineType] = None,
        project_id: typing.Optional[str] = None,
    ) -> typing.List[Pipeline]:
        """
        Search for pipelines by various parameters.

        Parameters:
            - project_name: typing.Optional[str].

            - pipeline_name: typing.Optional[str].

            - pipeline_type: typing.Optional[PipelineType].

            - project_id: typing.Optional[str].
        ---
        from llama_cloud import PipelineType
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.search_pipelines(
            pipeline_type=PipelineType.PLAYGROUND,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict(
                {
                    "project_name": project_name,
                    "pipeline_name": pipeline_name,
                    "pipeline_type": pipeline_type,
                    "project_id": project_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[Pipeline], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_pipeline(self, *, project_id: typing.Optional[str] = None, request: PipelineCreate) -> Pipeline:
        """
        Create a new pipeline for a project.

        Parameters:
            - project_id: typing.Optional[str].

            - request: PipelineCreate.
        ---
        from llama_cloud import (
            ConfigurableDataSinkNames,
            DataSinkCreate,
            EvalExecutionParams,
            FilterCondition,
            LlamaParseParameters,
            MetadataFilters,
            PipelineCreate,
            PipelineType,
            PresetRetrievalParams,
            RetrievalMode,
            SupportedLlmModelNames,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.create_pipeline(
            request=PipelineCreate(
                data_sink=DataSinkCreate(
                    name="string",
                    sink_type=ConfigurableDataSinkNames.CHROMA,
                ),
                preset_retrieval_parameters=PresetRetrievalParams(
                    search_filters=MetadataFilters(
                        filters=[],
                        condition=FilterCondition.AND,
                    ),
                    retrieval_mode=RetrievalMode.CHUNKS,
                ),
                eval_parameters=EvalExecutionParams(
                    llm_model=SupportedLlmModelNames.GPT_3_5_TURBO,
                ),
                llama_parse_parameters=LlamaParseParameters(),
                name="string",
                pipeline_type=PipelineType.PLAYGROUND,
            ),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict({"project_id": project_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert_pipeline(self, *, project_id: typing.Optional[str] = None, request: PipelineCreate) -> Pipeline:
        """
        Upsert a pipeline for a project.
        Updates if a pipeline with the same name and project_id already exists. Otherwise, creates a new pipeline.

        Parameters:
            - project_id: typing.Optional[str].

            - request: PipelineCreate.
        ---
        from llama_cloud import (
            ConfigurableDataSinkNames,
            DataSinkCreate,
            EvalExecutionParams,
            FilterCondition,
            LlamaParseParameters,
            MetadataFilters,
            PipelineCreate,
            PipelineType,
            PresetRetrievalParams,
            RetrievalMode,
            SupportedLlmModelNames,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.upsert_pipeline(
            request=PipelineCreate(
                data_sink=DataSinkCreate(
                    name="string",
                    sink_type=ConfigurableDataSinkNames.CHROMA,
                ),
                preset_retrieval_parameters=PresetRetrievalParams(
                    search_filters=MetadataFilters(
                        filters=[],
                        condition=FilterCondition.AND,
                    ),
                    retrieval_mode=RetrievalMode.CHUNKS,
                ),
                eval_parameters=EvalExecutionParams(
                    llm_model=SupportedLlmModelNames.GPT_3_5_TURBO,
                ),
                llama_parse_parameters=LlamaParseParameters(),
                name="string",
                pipeline_type=PipelineType.PLAYGROUND,
            ),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict({"project_id": project_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Get a pipeline by ID for a given project.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_existing_pipeline(
        self,
        pipeline_id: str,
        *,
        embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineUpdateTransformConfig] = OMIT,
        configured_transformations: typing.Optional[typing.List[ConfiguredTransformationItem]] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        eval_parameters: typing.Optional[EvalExecutionParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        name: typing.Optional[str] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
    ) -> Pipeline:
        """
        Update an existing pipeline for a project.

        Parameters:
            - pipeline_id: str.

            - embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig]. Configuration for the embedding model.

            - transform_config: typing.Optional[PipelineUpdateTransformConfig]. Configuration for the transformation.

            - configured_transformations: typing.Optional[typing.List[ConfiguredTransformationItem]]. Deprecated, use embedding_config or transform_config instead. configured transformations for the pipeline.

            - data_sink_id: typing.Optional[str]. Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

            - data_sink: typing.Optional[DataSinkCreate]. Data sink. When provided instead of data_sink_id, the data sink will be created.

            - preset_retrieval_parameters: typing.Optional[PresetRetrievalParams]. Preset retrieval parameters for the pipeline.

            - eval_parameters: typing.Optional[EvalExecutionParams]. Eval parameters for the pipeline.

            - llama_parse_parameters: typing.Optional[LlamaParseParameters]. Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

            - name: typing.Optional[str].

            - managed_pipeline_id: typing.Optional[str]. The ID of the ManagedPipeline this playground pipeline is linked to.
        ---
        from llama_cloud import (
            ConfigurableDataSinkNames,
            DataSinkCreate,
            EvalExecutionParams,
            FilterCondition,
            LlamaParseParameters,
            MetadataFilters,
            PresetRetrievalParams,
            RetrievalMode,
            SupportedLlmModelNames,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.update_existing_pipeline(
            pipeline_id="string",
            data_sink=DataSinkCreate(
                name="string",
                sink_type=ConfigurableDataSinkNames.CHROMA,
            ),
            preset_retrieval_parameters=PresetRetrievalParams(
                search_filters=MetadataFilters(
                    filters=[],
                    condition=FilterCondition.AND,
                ),
                retrieval_mode=RetrievalMode.CHUNKS,
            ),
            eval_parameters=EvalExecutionParams(
                llm_model=SupportedLlmModelNames.GPT_3_5_TURBO,
            ),
            llama_parse_parameters=LlamaParseParameters(),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if embedding_config is not OMIT:
            _request["embedding_config"] = embedding_config
        if transform_config is not OMIT:
            _request["transform_config"] = transform_config
        if configured_transformations is not OMIT:
            _request["configured_transformations"] = configured_transformations
        if data_sink_id is not OMIT:
            _request["data_sink_id"] = data_sink_id
        if data_sink is not OMIT:
            _request["data_sink"] = data_sink
        if preset_retrieval_parameters is not OMIT:
            _request["preset_retrieval_parameters"] = preset_retrieval_parameters
        if eval_parameters is not OMIT:
            _request["eval_parameters"] = eval_parameters
        if llama_parse_parameters is not OMIT:
            _request["llama_parse_parameters"] = llama_parse_parameters
        if name is not OMIT:
            _request["name"] = name
        if managed_pipeline_id is not OMIT:
            _request["managed_pipeline_id"] = managed_pipeline_id
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline(self, pipeline_id: str) -> None:
        """
        Delete a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_status(self, pipeline_id: str) -> ManagedIngestionStatusResponse:
        """
        Get the status of a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_status(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/status"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def sync_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Run ingestion for the pipeline by incrementally updating the data-sink with upstream changes from data-sources & files.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.sync_pipeline(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/sync"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def copy_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Copy a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.copy_pipeline(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/copy"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_eval_dataset_executions(self, pipeline_id: str, eval_dataset_id: str) -> typing.List[EvalDatasetJobRecord]:
        """
        Get the status of an EvalDatasetExecution.

        Parameters:
            - pipeline_id: str.

            - eval_dataset_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_eval_dataset_executions(
            pipeline_id="string",
            eval_dataset_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/eval-datasets/{eval_dataset_id}/execute",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[EvalDatasetJobRecord], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def execute_eval_dataset(
        self,
        pipeline_id: str,
        eval_dataset_id: str,
        *,
        eval_question_ids: typing.List[str],
        params: typing.Optional[EvalExecutionParamsOverride] = OMIT,
    ) -> EvalDatasetJobRecord:
        """
        Execute a dataset.

        Parameters:
            - pipeline_id: str.

            - eval_dataset_id: str.

            - eval_question_ids: typing.List[str].

            - params: typing.Optional[EvalExecutionParamsOverride]. The parameters for the eval execution that will override the ones set in the pipeline.
        ---
        from llama_cloud import EvalExecutionParamsOverride, SupportedLlmModelNames
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.execute_eval_dataset(
            pipeline_id="string",
            eval_dataset_id="string",
            eval_question_ids=[],
            params=EvalExecutionParamsOverride(
                llm_model=SupportedLlmModelNames.GPT_3_5_TURBO,
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {"eval_question_ids": eval_question_ids}
        if params is not OMIT:
            _request["params"] = params
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/eval-datasets/{eval_dataset_id}/execute",
            ),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(EvalDatasetJobRecord, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_eval_dataset_execution_result(
        self, pipeline_id: str, eval_dataset_id: str
    ) -> typing.List[EvalQuestionResult]:
        """
        Get the result of an EvalDatasetExecution.
        If eval_question_ids is specified, only the results for the specified
        questions will be returned.
        If any of the specified questions do not have a result, they will be ignored.

        Parameters:
            - pipeline_id: str.

            - eval_dataset_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_eval_dataset_execution_result(
            pipeline_id="string",
            eval_dataset_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/eval-datasets/{eval_dataset_id}/execute/result",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[EvalQuestionResult], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_eval_dataset_execution(
        self, pipeline_id: str, eval_dataset_id: str, eval_dataset_execution_id: str
    ) -> EvalDatasetJobRecord:
        """
        Get the status of an EvalDatasetExecution.

        Parameters:
            - pipeline_id: str.

            - eval_dataset_id: str.

            - eval_dataset_execution_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_eval_dataset_execution(
            pipeline_id="string",
            eval_dataset_id="string",
            eval_dataset_execution_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/eval-datasets/{eval_dataset_id}/execute/{eval_dataset_execution_id}",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(EvalDatasetJobRecord, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_files(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
    ) -> typing.List[PipelineFile]:
        """
        Get files for a pipeline.

        Parameters:
            - pipeline_id: str.

            - data_source_id: typing.Optional[str].

            - only_manually_uploaded: typing.Optional[bool].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_files(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files"),
            params=remove_none_from_dict(
                {"data_source_id": data_source_id, "only_manually_uploaded": only_manually_uploaded}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineFile], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def add_files_to_pipeline(
        self, pipeline_id: str, *, request: typing.List[PipelineFileCreate]
    ) -> typing.List[PipelineFile]:
        """
        Add files to a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[PipelineFileCreate].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.add_files_to_pipeline(
            pipeline_id="string",
            request=[],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files"),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineFile], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_file_status(self, pipeline_id: str, file_id: str) -> ManagedIngestionStatusResponse:
        """
        Get status of a file for a pipeline.

        Parameters:
            - pipeline_id: str.

            - file_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_file_status(
            pipeline_id="string",
            file_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files/{file_id}/status"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_pipeline_file(
        self,
        pipeline_id: str,
        file_id: str,
        *,
        custom_metadata: typing.Optional[typing.Dict[str, PipelineFileUpdateCustomMetadataValue]] = OMIT,
    ) -> PipelineFile:
        """
        Update a file for a pipeline.

        Parameters:
            - pipeline_id: str.

            - file_id: str.

            - custom_metadata: typing.Optional[typing.Dict[str, PipelineFileUpdateCustomMetadataValue]]. Custom metadata for the file
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.update_pipeline_file(
            pipeline_id="string",
            file_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if custom_metadata is not OMIT:
            _request["custom_metadata"] = custom_metadata
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files/{file_id}"
            ),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PipelineFile, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline_file(self, pipeline_id: str, file_id: str) -> None:
        """
        Delete a file from a pipeline.

        Parameters:
            - pipeline_id: str.

            - file_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline_file(
            pipeline_id="string",
            file_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files/{file_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def import_pipeline_metadata(self, pipeline_id: str, *, upload_file: typing.IO) -> typing.Dict[str, str]:
        """
        Import metadata for a pipeline.

        Parameters:
            - pipeline_id: str.

            - upload_file: typing.IO.
        """
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/metadata"),
            data=jsonable_encoder({}),
            files={"upload_file": upload_file},
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Dict[str, str], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline_files_metadata(self, pipeline_id: str) -> None:
        """
        Delete metadata for all files in a pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline_files_metadata(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/metadata"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_data_sources(self, pipeline_id: str) -> typing.List[PipelineDataSource]:
        """
        Get data sources for a pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_data_sources(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/data-sources"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineDataSource], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def add_data_sources_to_pipeline(
        self, pipeline_id: str, *, request: typing.List[PipelineDataSourceCreate]
    ) -> typing.List[PipelineDataSource]:
        """
        Add data sources to a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[PipelineDataSourceCreate].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.add_data_sources_to_pipeline(
            pipeline_id="string",
            request=[],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/data-sources"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineDataSource], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_pipeline_data_source(
        self, pipeline_id: str, data_source_id: str, *, sync_interval: typing.Optional[float] = OMIT
    ) -> PipelineDataSource:
        """
        Update the configuration of a data source in a pipeline.

        Parameters:
            - pipeline_id: str.

            - data_source_id: str.

            - sync_interval: typing.Optional[float]. The interval at which the data source should be synced.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.update_pipeline_data_source(
            pipeline_id="string",
            data_source_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if sync_interval is not OMIT:
            _request["sync_interval"] = sync_interval
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/data-sources/{data_source_id}",
            ),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PipelineDataSource, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline_data_source(self, pipeline_id: str, data_source_id: str) -> None:
        """
        Delete a data source from a pipeline.

        Parameters:
            - pipeline_id: str.

            - data_source_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline_data_source(
            pipeline_id="string",
            data_source_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/data-sources/{data_source_id}",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def sync_pipeline_data_source(self, pipeline_id: str, data_source_id: str) -> Pipeline:
        """
        Run ingestion for the pipeline data source by incrementally updating the data-sink with upstream changes from data-source.

        Parameters:
            - pipeline_id: str.

            - data_source_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.sync_pipeline_data_source(
            pipeline_id="string",
            data_source_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/data-sources/{data_source_id}/sync",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_data_source_status(self, pipeline_id: str, data_source_id: str) -> ManagedIngestionStatusResponse:
        """
        Get the status of a data source for a pipeline.

        Parameters:
            - pipeline_id: str.

            - data_source_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_data_source_status(
            pipeline_id="string",
            data_source_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/data-sources/{data_source_id}/status",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def run_search(
        self,
        pipeline_id: str,
        *,
        dense_similarity_top_k: typing.Optional[int] = OMIT,
        sparse_similarity_top_k: typing.Optional[int] = OMIT,
        enable_reranking: typing.Optional[bool] = OMIT,
        rerank_top_n: typing.Optional[int] = OMIT,
        alpha: typing.Optional[float] = OMIT,
        search_filters: typing.Optional[MetadataFilters] = OMIT,
        files_top_k: typing.Optional[int] = OMIT,
        retrieval_mode: typing.Optional[RetrievalMode] = OMIT,
        query: str,
    ) -> RetrieveResults:
        """
        Get retrieval results for a managed pipeline and a query

        Parameters:
            - pipeline_id: str.

            - dense_similarity_top_k: typing.Optional[int]. Number of nodes for dense retrieval.

            - sparse_similarity_top_k: typing.Optional[int]. Number of nodes for sparse retrieval.

            - enable_reranking: typing.Optional[bool]. Enable reranking for retrieval

            - rerank_top_n: typing.Optional[int]. Number of reranked nodes for returning.

            - alpha: typing.Optional[float]. Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval.

            - search_filters: typing.Optional[MetadataFilters]. Search filters for retrieval.

            - files_top_k: typing.Optional[int]. Number of files to retrieve (only for retrieval mode files_via_metadata and files_via_content).

            - retrieval_mode: typing.Optional[RetrievalMode]. The retrieval mode for the query.

            - query: str. The query to retrieve against.
        ---
        from llama_cloud import FilterCondition, MetadataFilters, RetrievalMode
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.run_search(
            pipeline_id="string",
            search_filters=MetadataFilters(
                filters=[],
                condition=FilterCondition.AND,
            ),
            retrieval_mode=RetrievalMode.CHUNKS,
            query="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"query": query}
        if dense_similarity_top_k is not OMIT:
            _request["dense_similarity_top_k"] = dense_similarity_top_k
        if sparse_similarity_top_k is not OMIT:
            _request["sparse_similarity_top_k"] = sparse_similarity_top_k
        if enable_reranking is not OMIT:
            _request["enable_reranking"] = enable_reranking
        if rerank_top_n is not OMIT:
            _request["rerank_top_n"] = rerank_top_n
        if alpha is not OMIT:
            _request["alpha"] = alpha
        if search_filters is not OMIT:
            _request["search_filters"] = search_filters
        if files_top_k is not OMIT:
            _request["files_top_k"] = files_top_k
        if retrieval_mode is not OMIT:
            _request["retrieval_mode"] = retrieval_mode
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/retrieve"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(RetrieveResults, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_jobs(self, pipeline_id: str) -> typing.List[PipelineDeployment]:
        """
        Get jobs for a pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_jobs(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/jobs"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineDeployment], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_job(self, pipeline_id: str, job_id: str) -> PipelineDeployment:
        """
        Get a job for a pipeline.

        Parameters:
            - pipeline_id: str.

            - job_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_job(
            pipeline_id="string",
            job_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/jobs/{job_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PipelineDeployment, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_playground_session(self, pipeline_id: str) -> PlaygroundSession:
        """
        Get a playground session for a user and pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_playground_session(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/playground-session"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PlaygroundSession, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def chat(
        self,
        pipeline_id: str,
        *,
        messages: typing.Optional[typing.List[InputMessage]] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        class_name: typing.Optional[str] = OMIT,
    ) -> typing.Any:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters:
            - pipeline_id: str.

            - messages: typing.Optional[typing.List[InputMessage]].

            - data: typing.Optional[ChatData].

            - class_name: typing.Optional[str].
        ---
        from llama_cloud import (
            ChatData,
            FilterCondition,
            LlmParameters,
            MetadataFilters,
            PresetRetrievalParams,
            RetrievalMode,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.chat(
            pipeline_id="string",
            data=ChatData(
                retrieval_parameters=PresetRetrievalParams(
                    search_filters=MetadataFilters(
                        filters=[],
                        condition=FilterCondition.AND,
                    ),
                    retrieval_mode=RetrievalMode.CHUNKS,
                ),
                llm_parameters=LlmParameters(),
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if messages is not OMIT:
            _request["messages"] = messages
        if data is not OMIT:
            _request["data"] = data
        if class_name is not OMIT:
            _request["class_name"] = class_name
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/chat"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
    ) -> typing.List[CloudDocument]:
        """
        Return a list of documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - skip: typing.Optional[int].

            - limit: typing.Optional[int].

            - file_id: typing.Optional[str].

            - only_direct_upload: typing.Optional[bool].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_documents(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            params=remove_none_from_dict(
                {"skip": skip, "limit": limit, "file_id": file_id, "only_direct_upload": only_direct_upload}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_batch_pipeline_documents(
        self, pipeline_id: str, *, request: typing.List[CloudDocumentCreate]
    ) -> typing.List[CloudDocument]:
        """
        Batch create documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[CloudDocumentCreate].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.create_batch_pipeline_documents(
            pipeline_id="string",
            request=[],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert_batch_pipeline_documents(
        self, pipeline_id: str, *, request: typing.List[CloudDocumentCreate]
    ) -> typing.List[CloudDocument]:
        """
        Batch create or update a document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[CloudDocumentCreate].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.upsert_batch_pipeline_documents(
            pipeline_id="string",
            request=[],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_document(self, pipeline_id: str, document_id: str) -> CloudDocument:
        """
        Return a single document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - document_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_document(
            pipeline_id="string",
            document_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/{document_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(CloudDocument, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline_document(self, pipeline_id: str, document_id: str) -> None:
        """
        Delete a document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - document_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline_document(
            pipeline_id="string",
            document_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/{document_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_document_status(self, pipeline_id: str, document_id: str) -> ManagedIngestionStatusResponse:
        """
        Return a single document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - document_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_document_status(
            pipeline_id="string",
            document_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/status",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_document_chunks(self, pipeline_id: str, document_id: str) -> typing.List[TextNode]:
        """
        Return a list of chunks for a pipeline document.

        Parameters:
            - pipeline_id: str.

            - document_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_document_chunks(
            pipeline_id="string",
            document_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/chunks",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[TextNode], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncPipelinesClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def search_pipelines(
        self,
        *,
        project_name: typing.Optional[str] = None,
        pipeline_name: typing.Optional[str] = None,
        pipeline_type: typing.Optional[PipelineType] = None,
        project_id: typing.Optional[str] = None,
    ) -> typing.List[Pipeline]:
        """
        Search for pipelines by various parameters.

        Parameters:
            - project_name: typing.Optional[str].

            - pipeline_name: typing.Optional[str].

            - pipeline_type: typing.Optional[PipelineType].

            - project_id: typing.Optional[str].
        ---
        from llama_cloud import PipelineType
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.search_pipelines(
            pipeline_type=PipelineType.PLAYGROUND,
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict(
                {
                    "project_name": project_name,
                    "pipeline_name": pipeline_name,
                    "pipeline_type": pipeline_type,
                    "project_id": project_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[Pipeline], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_pipeline(self, *, project_id: typing.Optional[str] = None, request: PipelineCreate) -> Pipeline:
        """
        Create a new pipeline for a project.

        Parameters:
            - project_id: typing.Optional[str].

            - request: PipelineCreate.
        ---
        from llama_cloud import (
            ConfigurableDataSinkNames,
            DataSinkCreate,
            EvalExecutionParams,
            FilterCondition,
            LlamaParseParameters,
            MetadataFilters,
            PipelineCreate,
            PipelineType,
            PresetRetrievalParams,
            RetrievalMode,
            SupportedLlmModelNames,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.create_pipeline(
            request=PipelineCreate(
                data_sink=DataSinkCreate(
                    name="string",
                    sink_type=ConfigurableDataSinkNames.CHROMA,
                ),
                preset_retrieval_parameters=PresetRetrievalParams(
                    search_filters=MetadataFilters(
                        filters=[],
                        condition=FilterCondition.AND,
                    ),
                    retrieval_mode=RetrievalMode.CHUNKS,
                ),
                eval_parameters=EvalExecutionParams(
                    llm_model=SupportedLlmModelNames.GPT_3_5_TURBO,
                ),
                llama_parse_parameters=LlamaParseParameters(),
                name="string",
                pipeline_type=PipelineType.PLAYGROUND,
            ),
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict({"project_id": project_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert_pipeline(self, *, project_id: typing.Optional[str] = None, request: PipelineCreate) -> Pipeline:
        """
        Upsert a pipeline for a project.
        Updates if a pipeline with the same name and project_id already exists. Otherwise, creates a new pipeline.

        Parameters:
            - project_id: typing.Optional[str].

            - request: PipelineCreate.
        ---
        from llama_cloud import (
            ConfigurableDataSinkNames,
            DataSinkCreate,
            EvalExecutionParams,
            FilterCondition,
            LlamaParseParameters,
            MetadataFilters,
            PipelineCreate,
            PipelineType,
            PresetRetrievalParams,
            RetrievalMode,
            SupportedLlmModelNames,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.upsert_pipeline(
            request=PipelineCreate(
                data_sink=DataSinkCreate(
                    name="string",
                    sink_type=ConfigurableDataSinkNames.CHROMA,
                ),
                preset_retrieval_parameters=PresetRetrievalParams(
                    search_filters=MetadataFilters(
                        filters=[],
                        condition=FilterCondition.AND,
                    ),
                    retrieval_mode=RetrievalMode.CHUNKS,
                ),
                eval_parameters=EvalExecutionParams(
                    llm_model=SupportedLlmModelNames.GPT_3_5_TURBO,
                ),
                llama_parse_parameters=LlamaParseParameters(),
                name="string",
                pipeline_type=PipelineType.PLAYGROUND,
            ),
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict({"project_id": project_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Get a pipeline by ID for a given project.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_existing_pipeline(
        self,
        pipeline_id: str,
        *,
        embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineUpdateTransformConfig] = OMIT,
        configured_transformations: typing.Optional[typing.List[ConfiguredTransformationItem]] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        eval_parameters: typing.Optional[EvalExecutionParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        name: typing.Optional[str] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
    ) -> Pipeline:
        """
        Update an existing pipeline for a project.

        Parameters:
            - pipeline_id: str.

            - embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig]. Configuration for the embedding model.

            - transform_config: typing.Optional[PipelineUpdateTransformConfig]. Configuration for the transformation.

            - configured_transformations: typing.Optional[typing.List[ConfiguredTransformationItem]]. Deprecated, use embedding_config or transform_config instead. configured transformations for the pipeline.

            - data_sink_id: typing.Optional[str]. Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

            - data_sink: typing.Optional[DataSinkCreate]. Data sink. When provided instead of data_sink_id, the data sink will be created.

            - preset_retrieval_parameters: typing.Optional[PresetRetrievalParams]. Preset retrieval parameters for the pipeline.

            - eval_parameters: typing.Optional[EvalExecutionParams]. Eval parameters for the pipeline.

            - llama_parse_parameters: typing.Optional[LlamaParseParameters]. Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

            - name: typing.Optional[str].

            - managed_pipeline_id: typing.Optional[str]. The ID of the ManagedPipeline this playground pipeline is linked to.
        ---
        from llama_cloud import (
            ConfigurableDataSinkNames,
            DataSinkCreate,
            EvalExecutionParams,
            FilterCondition,
            LlamaParseParameters,
            MetadataFilters,
            PresetRetrievalParams,
            RetrievalMode,
            SupportedLlmModelNames,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.update_existing_pipeline(
            pipeline_id="string",
            data_sink=DataSinkCreate(
                name="string",
                sink_type=ConfigurableDataSinkNames.CHROMA,
            ),
            preset_retrieval_parameters=PresetRetrievalParams(
                search_filters=MetadataFilters(
                    filters=[],
                    condition=FilterCondition.AND,
                ),
                retrieval_mode=RetrievalMode.CHUNKS,
            ),
            eval_parameters=EvalExecutionParams(
                llm_model=SupportedLlmModelNames.GPT_3_5_TURBO,
            ),
            llama_parse_parameters=LlamaParseParameters(),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if embedding_config is not OMIT:
            _request["embedding_config"] = embedding_config
        if transform_config is not OMIT:
            _request["transform_config"] = transform_config
        if configured_transformations is not OMIT:
            _request["configured_transformations"] = configured_transformations
        if data_sink_id is not OMIT:
            _request["data_sink_id"] = data_sink_id
        if data_sink is not OMIT:
            _request["data_sink"] = data_sink
        if preset_retrieval_parameters is not OMIT:
            _request["preset_retrieval_parameters"] = preset_retrieval_parameters
        if eval_parameters is not OMIT:
            _request["eval_parameters"] = eval_parameters
        if llama_parse_parameters is not OMIT:
            _request["llama_parse_parameters"] = llama_parse_parameters
        if name is not OMIT:
            _request["name"] = name
        if managed_pipeline_id is not OMIT:
            _request["managed_pipeline_id"] = managed_pipeline_id
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline(self, pipeline_id: str) -> None:
        """
        Delete a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.delete_pipeline(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_status(self, pipeline_id: str) -> ManagedIngestionStatusResponse:
        """
        Get the status of a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline_status(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/status"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def sync_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Run ingestion for the pipeline by incrementally updating the data-sink with upstream changes from data-sources & files.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.sync_pipeline(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/sync"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def copy_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Copy a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.copy_pipeline(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/copy"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_eval_dataset_executions(
        self, pipeline_id: str, eval_dataset_id: str
    ) -> typing.List[EvalDatasetJobRecord]:
        """
        Get the status of an EvalDatasetExecution.

        Parameters:
            - pipeline_id: str.

            - eval_dataset_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_eval_dataset_executions(
            pipeline_id="string",
            eval_dataset_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/eval-datasets/{eval_dataset_id}/execute",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[EvalDatasetJobRecord], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def execute_eval_dataset(
        self,
        pipeline_id: str,
        eval_dataset_id: str,
        *,
        eval_question_ids: typing.List[str],
        params: typing.Optional[EvalExecutionParamsOverride] = OMIT,
    ) -> EvalDatasetJobRecord:
        """
        Execute a dataset.

        Parameters:
            - pipeline_id: str.

            - eval_dataset_id: str.

            - eval_question_ids: typing.List[str].

            - params: typing.Optional[EvalExecutionParamsOverride]. The parameters for the eval execution that will override the ones set in the pipeline.
        ---
        from llama_cloud import EvalExecutionParamsOverride, SupportedLlmModelNames
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.execute_eval_dataset(
            pipeline_id="string",
            eval_dataset_id="string",
            eval_question_ids=[],
            params=EvalExecutionParamsOverride(
                llm_model=SupportedLlmModelNames.GPT_3_5_TURBO,
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {"eval_question_ids": eval_question_ids}
        if params is not OMIT:
            _request["params"] = params
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/eval-datasets/{eval_dataset_id}/execute",
            ),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(EvalDatasetJobRecord, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_eval_dataset_execution_result(
        self, pipeline_id: str, eval_dataset_id: str
    ) -> typing.List[EvalQuestionResult]:
        """
        Get the result of an EvalDatasetExecution.
        If eval_question_ids is specified, only the results for the specified
        questions will be returned.
        If any of the specified questions do not have a result, they will be ignored.

        Parameters:
            - pipeline_id: str.

            - eval_dataset_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_eval_dataset_execution_result(
            pipeline_id="string",
            eval_dataset_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/eval-datasets/{eval_dataset_id}/execute/result",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[EvalQuestionResult], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_eval_dataset_execution(
        self, pipeline_id: str, eval_dataset_id: str, eval_dataset_execution_id: str
    ) -> EvalDatasetJobRecord:
        """
        Get the status of an EvalDatasetExecution.

        Parameters:
            - pipeline_id: str.

            - eval_dataset_id: str.

            - eval_dataset_execution_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_eval_dataset_execution(
            pipeline_id="string",
            eval_dataset_id="string",
            eval_dataset_execution_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/eval-datasets/{eval_dataset_id}/execute/{eval_dataset_execution_id}",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(EvalDatasetJobRecord, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_files(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
    ) -> typing.List[PipelineFile]:
        """
        Get files for a pipeline.

        Parameters:
            - pipeline_id: str.

            - data_source_id: typing.Optional[str].

            - only_manually_uploaded: typing.Optional[bool].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.list_pipeline_files(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files"),
            params=remove_none_from_dict(
                {"data_source_id": data_source_id, "only_manually_uploaded": only_manually_uploaded}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineFile], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def add_files_to_pipeline(
        self, pipeline_id: str, *, request: typing.List[PipelineFileCreate]
    ) -> typing.List[PipelineFile]:
        """
        Add files to a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[PipelineFileCreate].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.add_files_to_pipeline(
            pipeline_id="string",
            request=[],
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files"),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineFile], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_file_status(self, pipeline_id: str, file_id: str) -> ManagedIngestionStatusResponse:
        """
        Get status of a file for a pipeline.

        Parameters:
            - pipeline_id: str.

            - file_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline_file_status(
            pipeline_id="string",
            file_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files/{file_id}/status"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_pipeline_file(
        self,
        pipeline_id: str,
        file_id: str,
        *,
        custom_metadata: typing.Optional[typing.Dict[str, PipelineFileUpdateCustomMetadataValue]] = OMIT,
    ) -> PipelineFile:
        """
        Update a file for a pipeline.

        Parameters:
            - pipeline_id: str.

            - file_id: str.

            - custom_metadata: typing.Optional[typing.Dict[str, PipelineFileUpdateCustomMetadataValue]]. Custom metadata for the file
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.update_pipeline_file(
            pipeline_id="string",
            file_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if custom_metadata is not OMIT:
            _request["custom_metadata"] = custom_metadata
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files/{file_id}"
            ),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PipelineFile, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline_file(self, pipeline_id: str, file_id: str) -> None:
        """
        Delete a file from a pipeline.

        Parameters:
            - pipeline_id: str.

            - file_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.delete_pipeline_file(
            pipeline_id="string",
            file_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/files/{file_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def import_pipeline_metadata(self, pipeline_id: str, *, upload_file: typing.IO) -> typing.Dict[str, str]:
        """
        Import metadata for a pipeline.

        Parameters:
            - pipeline_id: str.

            - upload_file: typing.IO.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/metadata"),
            data=jsonable_encoder({}),
            files={"upload_file": upload_file},
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Dict[str, str], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline_files_metadata(self, pipeline_id: str) -> None:
        """
        Delete metadata for all files in a pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.delete_pipeline_files_metadata(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/metadata"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_data_sources(self, pipeline_id: str) -> typing.List[PipelineDataSource]:
        """
        Get data sources for a pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.list_pipeline_data_sources(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/data-sources"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineDataSource], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def add_data_sources_to_pipeline(
        self, pipeline_id: str, *, request: typing.List[PipelineDataSourceCreate]
    ) -> typing.List[PipelineDataSource]:
        """
        Add data sources to a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[PipelineDataSourceCreate].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.add_data_sources_to_pipeline(
            pipeline_id="string",
            request=[],
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/data-sources"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineDataSource], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_pipeline_data_source(
        self, pipeline_id: str, data_source_id: str, *, sync_interval: typing.Optional[float] = OMIT
    ) -> PipelineDataSource:
        """
        Update the configuration of a data source in a pipeline.

        Parameters:
            - pipeline_id: str.

            - data_source_id: str.

            - sync_interval: typing.Optional[float]. The interval at which the data source should be synced.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.update_pipeline_data_source(
            pipeline_id="string",
            data_source_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if sync_interval is not OMIT:
            _request["sync_interval"] = sync_interval
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/data-sources/{data_source_id}",
            ),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PipelineDataSource, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline_data_source(self, pipeline_id: str, data_source_id: str) -> None:
        """
        Delete a data source from a pipeline.

        Parameters:
            - pipeline_id: str.

            - data_source_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.delete_pipeline_data_source(
            pipeline_id="string",
            data_source_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/data-sources/{data_source_id}",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def sync_pipeline_data_source(self, pipeline_id: str, data_source_id: str) -> Pipeline:
        """
        Run ingestion for the pipeline data source by incrementally updating the data-sink with upstream changes from data-source.

        Parameters:
            - pipeline_id: str.

            - data_source_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.sync_pipeline_data_source(
            pipeline_id="string",
            data_source_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/data-sources/{data_source_id}/sync",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_data_source_status(
        self, pipeline_id: str, data_source_id: str
    ) -> ManagedIngestionStatusResponse:
        """
        Get the status of a data source for a pipeline.

        Parameters:
            - pipeline_id: str.

            - data_source_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline_data_source_status(
            pipeline_id="string",
            data_source_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/data-sources/{data_source_id}/status",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def run_search(
        self,
        pipeline_id: str,
        *,
        dense_similarity_top_k: typing.Optional[int] = OMIT,
        sparse_similarity_top_k: typing.Optional[int] = OMIT,
        enable_reranking: typing.Optional[bool] = OMIT,
        rerank_top_n: typing.Optional[int] = OMIT,
        alpha: typing.Optional[float] = OMIT,
        search_filters: typing.Optional[MetadataFilters] = OMIT,
        files_top_k: typing.Optional[int] = OMIT,
        retrieval_mode: typing.Optional[RetrievalMode] = OMIT,
        query: str,
    ) -> RetrieveResults:
        """
        Get retrieval results for a managed pipeline and a query

        Parameters:
            - pipeline_id: str.

            - dense_similarity_top_k: typing.Optional[int]. Number of nodes for dense retrieval.

            - sparse_similarity_top_k: typing.Optional[int]. Number of nodes for sparse retrieval.

            - enable_reranking: typing.Optional[bool]. Enable reranking for retrieval

            - rerank_top_n: typing.Optional[int]. Number of reranked nodes for returning.

            - alpha: typing.Optional[float]. Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval.

            - search_filters: typing.Optional[MetadataFilters]. Search filters for retrieval.

            - files_top_k: typing.Optional[int]. Number of files to retrieve (only for retrieval mode files_via_metadata and files_via_content).

            - retrieval_mode: typing.Optional[RetrievalMode]. The retrieval mode for the query.

            - query: str. The query to retrieve against.
        ---
        from llama_cloud import FilterCondition, MetadataFilters, RetrievalMode
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.run_search(
            pipeline_id="string",
            search_filters=MetadataFilters(
                filters=[],
                condition=FilterCondition.AND,
            ),
            retrieval_mode=RetrievalMode.CHUNKS,
            query="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"query": query}
        if dense_similarity_top_k is not OMIT:
            _request["dense_similarity_top_k"] = dense_similarity_top_k
        if sparse_similarity_top_k is not OMIT:
            _request["sparse_similarity_top_k"] = sparse_similarity_top_k
        if enable_reranking is not OMIT:
            _request["enable_reranking"] = enable_reranking
        if rerank_top_n is not OMIT:
            _request["rerank_top_n"] = rerank_top_n
        if alpha is not OMIT:
            _request["alpha"] = alpha
        if search_filters is not OMIT:
            _request["search_filters"] = search_filters
        if files_top_k is not OMIT:
            _request["files_top_k"] = files_top_k
        if retrieval_mode is not OMIT:
            _request["retrieval_mode"] = retrieval_mode
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/retrieve"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(RetrieveResults, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_jobs(self, pipeline_id: str) -> typing.List[PipelineDeployment]:
        """
        Get jobs for a pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.list_pipeline_jobs(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/jobs"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[PipelineDeployment], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_job(self, pipeline_id: str, job_id: str) -> PipelineDeployment:
        """
        Get a job for a pipeline.

        Parameters:
            - pipeline_id: str.

            - job_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline_job(
            pipeline_id="string",
            job_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/jobs/{job_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PipelineDeployment, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_playground_session(self, pipeline_id: str) -> PlaygroundSession:
        """
        Get a playground session for a user and pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_playground_session(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/playground-session"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PlaygroundSession, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def chat(
        self,
        pipeline_id: str,
        *,
        messages: typing.Optional[typing.List[InputMessage]] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        class_name: typing.Optional[str] = OMIT,
    ) -> typing.Any:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters:
            - pipeline_id: str.

            - messages: typing.Optional[typing.List[InputMessage]].

            - data: typing.Optional[ChatData].

            - class_name: typing.Optional[str].
        ---
        from llama_cloud import (
            ChatData,
            FilterCondition,
            LlmParameters,
            MetadataFilters,
            PresetRetrievalParams,
            RetrievalMode,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.chat(
            pipeline_id="string",
            data=ChatData(
                retrieval_parameters=PresetRetrievalParams(
                    search_filters=MetadataFilters(
                        filters=[],
                        condition=FilterCondition.AND,
                    ),
                    retrieval_mode=RetrievalMode.CHUNKS,
                ),
                llm_parameters=LlmParameters(),
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if messages is not OMIT:
            _request["messages"] = messages
        if data is not OMIT:
            _request["data"] = data
        if class_name is not OMIT:
            _request["class_name"] = class_name
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/chat"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
    ) -> typing.List[CloudDocument]:
        """
        Return a list of documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - skip: typing.Optional[int].

            - limit: typing.Optional[int].

            - file_id: typing.Optional[str].

            - only_direct_upload: typing.Optional[bool].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.list_pipeline_documents(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            params=remove_none_from_dict(
                {"skip": skip, "limit": limit, "file_id": file_id, "only_direct_upload": only_direct_upload}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_batch_pipeline_documents(
        self, pipeline_id: str, *, request: typing.List[CloudDocumentCreate]
    ) -> typing.List[CloudDocument]:
        """
        Batch create documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[CloudDocumentCreate].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.create_batch_pipeline_documents(
            pipeline_id="string",
            request=[],
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert_batch_pipeline_documents(
        self, pipeline_id: str, *, request: typing.List[CloudDocumentCreate]
    ) -> typing.List[CloudDocument]:
        """
        Batch create or update a document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[CloudDocumentCreate].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.upsert_batch_pipeline_documents(
            pipeline_id="string",
            request=[],
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_document(self, pipeline_id: str, document_id: str) -> CloudDocument:
        """
        Return a single document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - document_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline_document(
            pipeline_id="string",
            document_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/{document_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(CloudDocument, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline_document(self, pipeline_id: str, document_id: str) -> None:
        """
        Delete a document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - document_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.delete_pipeline_document(
            pipeline_id="string",
            document_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/{document_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_document_status(self, pipeline_id: str, document_id: str) -> ManagedIngestionStatusResponse:
        """
        Return a single document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - document_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline_document_status(
            pipeline_id="string",
            document_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/status",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_document_chunks(self, pipeline_id: str, document_id: str) -> typing.List[TextNode]:
        """
        Return a list of chunks for a pipeline document.

        Parameters:
            - pipeline_id: str.

            - document_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.list_pipeline_document_chunks(
            pipeline_id="string",
            document_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/chunks",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[TextNode], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
